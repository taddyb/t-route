import numpy as np
import pandas as pd
import yaml
from datetime import datetime, timedelta
from joblib import delayed, Parallel
import xarray as xr
import glob
import pathlib

import logging

# from bmi_array2df import *
# import bmi_array2df as a2df

# from bmi_df2array import *
import bmi_df2array as df2a

from troute.routing.fast_reach.reservoir_RFC_da import _validate_RFC_data
import netCDF4
from nwm_routing.log_level_set import log_level_set
from troute.config import Config

LOG = logging.getLogger("")


class DAforcing_model:
    def __init__(self, bmi_cfg_file=None):
        """ """
        __slots__ = [
            "_data_assimilation_parameters",
            "_forcing_parameters",
            "_compute_parameters",
            "_output_parameters",
            "_usgs_df",
            "reservoir_usgs_df",
            "reservoir_usace_df",
            "_rfc_timeseries_df",
            "_lastobs_df",
            "_t0",
            "_q0",
            "_waterbody_df",
            "_write_lite_restart",
            "_dateNull",
            "_datesSecondsArray_usgs",
            "_nDates_usgs",
            "_stationArray_usgs",
            "_stationStringLengthArray_usgs",
            "_nStations_usgs",
            "_usgs_Array",
            "_datesSecondsArray_reservoir_usgs",
            "_nDates_reservoir_usgs",
            "_stationArray_reservoir_usgs",
            "_stationStringLengthArray_reservoir_usgs",
            "_nStations_reservoir_usgs",
            "_usgs_reservoir_Array",
            "_datesSecondsArray_reservoir_usace",
            "_nDates_reservoir_usace",
            "_stationArray_reservoir_usace",
            "_stationStringLengthArray_reservoir_usace",
            "_nStations_reservoir_usace",
            "_usace_reservoir_Array",
            "_rfc_da_timestep",
            "_rfc_totalCounts",
            "_rfc_synthetic_values",
            "_rfc_discharges",
            "_rfc_timeseries_idx",
            "_rfc_use_rfc",
            "_rfc_Datetime",
            "_rfc_timeSteps",
            "_rfc_StationId_array",
            "_rfc_StationId_stringLengths",
            "_rfc_List_array",
            "_rfc_List_stringLengths",
            "_lastObs_gageArray",
            "_lastObs_gageStringLengths",
            "_lastObs_timeSince",
            "_lastObs_discharge",
            "_q0_columnArray",
            "_q0_columnLengthArray",
            "_q0_nCol",
            "_q0_indexArray",
            "_q0_nIndex",
            "_q0_Array",
            "_waterbodyLR_columnArray",
            "_waterbodyLR_columnLengthArray",
            "_waterbodyLR_nCol",
            "_waterbodyLR_indexArray",
            "_waterbodyLR_nIndex",
            "_waterbodyLR_Array",
        ]

        if bmi_cfg_file:
            (
                compute_parameters,
                forcing_parameters,
                data_assimilation_parameters,
                output_parameters,
            ) = _read_config_file(bmi_cfg_file)

            self._compute_parameters = compute_parameters
            self._forcing_parameters = forcing_parameters
            self._data_assimilation_parameters = data_assimilation_parameters
            self._output_parameters = output_parameters

            self._t0 = self._compute_parameters["restart_parameters"]["start_datetime"]
            self._write_lite_restart = 0
            #############################
            # Read DA files:
            #############################
            nudging = data_assimilation_parameters.get("streamflow_da", {}).get(
                "streamflow_nudging", False
            )

            usgs_persistence = (
                data_assimilation_parameters.get("reservoir_da", {})
                .get("reservoir_persistence_da", {})
                .get("reservoir_persistence_usgs", False)
            )
            usace_persistence = (
                data_assimilation_parameters.get("reservoir_da", {})
                .get("reservoir_persistence_da", {})
                .get("reservoir_persistence_usace", False)
            )
            rfc = (
                data_assimilation_parameters.get("reservoir_da", {})
                .get("reservoir_rfc_da", {})
                .get("reservoir_rfc_forecasts", False)
            )

            qc_threshold = data_assimilation_parameters.get("qc_threshold")
            cpu_pool = compute_parameters.get("cpu_pool")

            # Produce list of datetimes to search for timeslice files
            lookback_hrs = data_assimilation_parameters.get("timeslice_lookback_hours")
            start_datetime = compute_parameters.get("restart_parameters").get(
                "start_datetime"
            )
            dt = compute_parameters.get("forcing_parameters").get("dt")
            nts = compute_parameters.get("forcing_parameters").get("nts")
            timeslice_start = start_datetime - timedelta(hours=lookback_hrs)
            timeslice_end = start_datetime + timedelta(seconds=dt * nts)
            delta = timedelta(minutes=15)
            timeslice_dates = []
            while timeslice_start <= timeslice_end:
                timeslice_dates.append(timeslice_start.strftime("%Y-%m-%d_%H:%M:%S"))
                timeslice_start += delta

            # Create empty default dataframes:
            self._usgs_df = pd.DataFrame()
            self._reservoir_usgs_df = pd.DataFrame()
            self._reservoir_usace_df = pd.DataFrame()
            self._rfc_timeseries_df = pd.DataFrame()
            self._lastobs_df = pd.DataFrame()

            # USGS Observations
            if nudging or usgs_persistence:
                usgs_timeslice_path = str(
                    data_assimilation_parameters.get("usgs_timeslices_folder")
                )
                if nudging:
                    self._usgs_df = _read_timeslice_files(
                        usgs_timeslice_path,
                        timeslice_dates,
                        qc_threshold,
                        dt,
                        cpu_pool,
                    )
                    self._reservoir_usgs_df = (
                        self._usgs_df.transpose().resample("15min").asfreq().transpose()
                    )
                else:
                    self._usgs_df = pd.DataFrame()
                    self._reservoir_usgs_df = _read_timeslice_files(
                        usgs_timeslice_path,
                        timeslice_dates,
                        qc_threshold,
                        900,  # 15 minutes
                        cpu_pool,
                    )

            # USACE Observations
            if usace_persistence:
                usace_timeslice_path = str(
                    data_assimilation_parameters.get("usace_timeslices_folder")
                )
                self._reservoir_usace_df = _read_timeslice_files(
                    usace_timeslice_path,
                    timeslice_dates,
                    qc_threshold,
                    900,  # 15 minutes
                    cpu_pool,
                )

            # Produce list of datetimes to search for timeseries files
            rfc_parameters = data_assimilation_parameters.get("reservoir_da", {}).get(
                "reservoir_rfc_da", {}
            )
            lookback_hrs = rfc_parameters.get("reservoir_rfc_forecasts_lookback_hours")
            offset_hrs = rfc_parameters.get("reservoir_rfc_forecasts_offset_hours")
            timeseries_end = start_datetime + timedelta(hours=offset_hrs)
            timeseries_start = timeseries_end - timedelta(hours=lookback_hrs)
            delta = timedelta(hours=1)
            timeseries_dates = []
            while timeseries_start <= timeseries_end:
                timeseries_dates.append(timeseries_start.strftime("%Y-%m-%d_%H"))
                timeseries_start += delta
            rfc_forecast_persist_days = rfc_parameters.get(
                "reservoir_rfc_forecast_persist_days"
            )
            final_persist_datetime = start_datetime + timedelta(
                days=rfc_forecast_persist_days
            )

            # RFC Observations
            if rfc:
                rfc_timeseries_path = str(
                    rfc_parameters.get("reservoir_rfc_forecasts_time_series_path")
                )
                self._rfc_timeseries_df = _read_timeseries_files(
                    rfc_timeseries_path,
                    timeseries_dates,
                    start_datetime,
                    final_persist_datetime,
                )

            # Lastobs
            lastobs_file = data_assimilation_parameters.get("streamflow_da", {}).get(
                "lastobs_file", False
            )

            if lastobs_file:
                self._lastobs_df = _read_lastobs_file(lastobs_file)

            # save time reference: start_datetime
            self._dateNull = start_datetime

            # read in metadata for BMI compliant arrays:

            self._values = {}

            self._lastObs_gageArray = np.zeros(0)
            self._lastObs_gageStringLengths = np.zeros(0)
            self._lastObs_timeSince = np.zeros(0)
            self._lastObs_discharge = np.zeros(0)

            if not self._lastobs_df.empty:
                if lastobs_file:
                    (
                        _lastObs_gageArray,
                        _lastObs_gageStringLengths,
                        _lastObs_timeSince,
                        _lastObs_discharge,
                    ) = df2a._bmi_disassemble_lastObs(self._lastobs_df)

                    self._lastObs_gageArray = _lastObs_gageArray
                    self._lastObs_gageStringLengths = _lastObs_gageStringLengths
                    self._lastObs_timeSince = _lastObs_timeSince
                    self._lastObs_discharge = _lastObs_discharge

            # USGS Observations
            self._datesSecondsArray_usgs = np.zeros(0)
            self._nDates_usgs = np.zeros(0)
            self._stationArray_usgs = np.zeros(0)
            self._stationStringLengthArray_usgs = np.zeros(0)
            self._nStations_usgs = np.zeros(0)
            self._usgsArray = np.zeros(0)

            if not self._usgs_df.empty:
                #
                # USGS dataframe:
                #
                # Dates related:
                # - datesSecondsArray_usgs: "column names" of usgs dataframe: dates
                #        in seconds relative to start_datetime
                # - nDates_usgs: number of "dates" columns
                #
                # Station ID related:
                # - stationArray_usgs: "row names" of usgs dataframe: all station
                #        names decoded into arrays of one ASCII code per string,
                #        attached together
                # - stationStringLengthArray_usgs: array of lengths of station IDs,
                #        in case of mixed arrays (not all station IDs have equal lengths)
                # - nStations_usgs: number of stations
                #
                (
                    _datesSecondsArray_usgs,
                    _nDates_usgs,
                    _stationArray_usgs,
                    _stationStringLengthArray_usgs,
                    _nStations_usgs,
                ) = df2a._time_stations_from_df(self._usgs_df, start_datetime)
                # save metadata in class instances
                self._datesSecondsArray_usgs = _datesSecondsArray_usgs
                self._nDates_usgs = _nDates_usgs
                self._stationArray_usgs = _stationArray_usgs
                self._stationStringLengthArray_usgs = _stationStringLengthArray_usgs
                self._nStations_usgs = _nStations_usgs
                # flatten the actual USGS dataframe into a numpy ndarray
                _usgsArray = df2a._flatten_array(self._usgs_df, np.float32)
                # ... and save it with the class instance
                self._usgsArray = _usgsArray

            # USGS Reservoir Observations
            self._datesSecondsArray_reservoir_usgs = np.zeros(0)
            self._nDates_reservoir_usgs = np.zeros(0)
            self._stationArray_reservoir_usgs = np.zeros(0)
            self._stationStringLengthArray_reservoir_usgs = np.zeros(0)
            self._nStations_reservoir_usgs = np.zeros(0)
            self._reservoirUsgsArray = np.zeros(0)

            if not self._reservoir_usgs_df.empty:
                #
                # Reservoir USGS and USACE dataframes: Array structures of
                # _reservoir_usgs_df and _reservoir_usace_df are
                # fundamentally the same as usgs_df: input/output analogous.
                #
                # In the following comments, usgs shall also represent usace
                #
                # Dates related:
                # - datesSecondsArray_reservoir_usgs
                # - nDates_usgs
                #
                # Station ID related:
                # - stationArray_usgs
                # - stationStringLengthArray_usgs
                # - nStations_usgs
                #
                (
                    _datesSecondsArray_reservoir_usgs,
                    _nDates_reservoir_usgs,
                    _stationArray_reservoir_usgs,
                    _stationStringLengthArray_reservoir_usgs,
                    _nStations_reservoir_usgs,
                ) = df2a._time_stations_from_df(self._reservoir_usgs_df, start_datetime)
                # save metadata in class instances
                self._datesSecondsArray_reservoir_usgs = (
                    _datesSecondsArray_reservoir_usgs
                )
                self._nDates_reservoir_usgs = _nDates_reservoir_usgs
                self._stationArray_reservoir_usgs = _stationArray_reservoir_usgs
                self._stationStringLengthArray_reservoir_usgs = (
                    _stationStringLengthArray_reservoir_usgs
                )
                self._nStations_reservoir_usgs = _nStations_reservoir_usgs
                # flatten the actual USGS datafrane into a numpy ndarray
                _reservoirUsgsArray = df2a._flatten_array(
                    self._reservoir_usgs_df, np.float32
                )
                # ... and save it with the class instance
                self._reservoirUsgsArray = _reservoirUsgsArray

            # USACE Reservoir Observations
            self._datesSecondsArray_reservoir_usace = np.zeros(0)
            self._nDates_reservoir_usace = np.zeros(0)
            self._stationArray_reservoir_usace = np.zeros(0)
            self._stationStringLengthArray_reservoir_usace = np.zeros(0)
            self._nStations_reservoir_usace = np.zeros(0)
            self._reservoirUsaceArray = np.zeros(0)

            if not self._reservoir_usace_df.empty:
                # see detailed comments in USGS branch
                (
                    _datesSecondsArray_reservoir_usace,
                    _nDates_reservoir_usace,
                    _stationArray_reservoir_usace,
                    _stationStringLengthArray_reservoir_usace,
                    _nStations_reservoir_usace,
                ) = df2a._time_stations_from_df(
                    self._reservoir_usace_df, start_datetime
                )
                # save metadata in class instances
                self._datesSecondsArray_reservoir_usace = (
                    _datesSecondsArray_reservoir_usace
                )
                self._nDates_reservoir_usace = _nDates_reservoir_usace
                self._stationArray_reservoir_usace = _stationArray_reservoir_usace
                self._stationStringLengthArray_reservoir_usace = (
                    _stationStringLengthArray_reservoir_usace
                )
                self._nStations_reservoir_usace = _nStations_reservoir_usace
                # flatten the actual USACE datafrane into a numpy ndarray
                _reservoirUsaceArray = df2a._flatten_array(
                    self._reservoir_usace_df, np.float32
                )
                # ... and save it with the class instance
                self._reservoirUsaceArray = _reservoirUsaceArray

            # RFC Timeseries
            self._rfc_da_timestep = np.zeros(0)
            self._rfc_totalCounts = np.zeros(0)
            self._rfc_synthetic_values = np.zeros(0)
            self._rfc_discharges = np.zeros(0)
            self._rfc_timeseries_idx = np.zeros(0)
            self._rfc_use_rfc = np.zeros(0)
            self._rfc_Datetime = np.zeros(0)
            self._rfc_timeSteps = np.zeros(0)
            self._rfc_StationId_array = np.zeros(0)
            self._rfc_StationId_stringLengths = np.zeros(0)
            self._rfc_List_array = np.zeros(0)
            self._rfc_List_stringLengths = np.zeros(0)

            if not self._rfc_timeseries_df.empty:
                (
                    _rfc_da_timestep,
                    _rfc_totalCounts,
                    _rfc_synthetic_values,
                    _rfc_discharges,
                    _rfc_timeseries_idx,
                    _rfc_use_rfc,
                    _rfc_Datetime,
                    _rfc_timeSteps,
                    _rfc_StationId_array,
                    _rfc_StationId_stringLengths,
                    _rfc_List_array,
                    _rfc_List_stringLengths,
                ) = df2a._bmi_disassemble_rfc_timeseries(
                    self._rfc_timeseries_df, start_datetime
                )
                # save all data in class instance
                self._rfc_da_timestep = _rfc_da_timestep
                self._rfc_totalCounts = _rfc_totalCounts
                self._rfc_synthetic_values = _rfc_synthetic_values
                self._rfc_discharges = _rfc_discharges
                self._rfc_timeseries_idx = _rfc_timeseries_idx
                self._rfc_use_rfc = _rfc_use_rfc
                self._rfc_Datetime = _rfc_Datetime
                self._rfc_timeSteps = _rfc_timeSteps
                self._rfc_StationId_array = _rfc_StationId_array
                self._rfc_StationId_stringLengths = _rfc_StationId_stringLengths
                self._rfc_List_array = _rfc_List_array
                self._rfc_List_stringLengths = _rfc_List_stringLengths

            #############################
            # Read Restart files:
            #############################

            # Create empty default dataframes:
            self._q0 = pd.DataFrame()
            self._waterbody_df = pd.DataFrame()

            # q0
            self._q0_columnArray = np.zeros(0)
            self._q0_columnLengthArray = np.zeros(0)
            self._q0_nCol = np.zeros(0)
            self._q0_indexArray = np.zeros(0)
            self._q0_nIndex = np.zeros(0)
            self._q0_Array = np.zeros(0)

            # waterbody_df
            self._waterbodyLR_columnArray = np.zeros(0)
            self._waterbodyLR_columnLengthArray = np.zeros(0)
            self._waterbodyLR_nCol = np.zeros(0)
            self._waterbodyLR_indexArray = np.zeros(0)
            self._waterbodyLR_nIndex = np.zeros(0)
            self._waterbodyLR_Array = np.zeros(0)

            lite_restart_file = self._compute_parameters["restart_parameters"][
                "lite_channel_restart_file"
            ]
            if lite_restart_file:
                self._q0, self._t0 = _read_lite_restart(lite_restart_file)

                if not self._q0.empty:
                    (
                        _q0_columnArray,
                        _q0_columnLengthArray,
                        _q0_nCol,
                        _q0_indexArray,
                        _q0_nIndex,
                        _q0_Array,
                    ) = df2a._bmi_disassemble_lite_restart(self._q0, np.float32)

                    self._q0_columnArray = _q0_columnArray
                    self._q0_columnLengthArray = _q0_columnLengthArray
                    self._q0_nCol = _q0_nCol
                    self._q0_indexArray = _q0_indexArray
                    self._q0_nIndex = _q0_nIndex
                    self._q0_Array = _q0_Array

            lite_restart_file = self._compute_parameters["restart_parameters"][
                "lite_waterbody_restart_file"
            ]

            if lite_restart_file:
                self._waterbody_df, _ = _read_lite_restart(lite_restart_file)

                if not self._waterbody_df.empty:
                    (
                        _waterbodyLR_columnArray,
                        _waterbodyLR_columnLengthArray,
                        _waterbodyLR_nCol,
                        _waterbodyLR_indexArray,
                        _waterbodyLR_nIndex,
                        _waterbodyLR_Array,
                    ) = df2a._bmi_disassemble_lite_restart(
                        self._waterbody_df, np.float64
                    )

                    self._waterbodyLR_columnArray = _waterbodyLR_columnArray
                    self._waterbodyLR_columnLengthArray = _waterbodyLR_columnLengthArray
                    self._waterbodyLR_nCol = _waterbodyLR_nCol
                    self._waterbodyLR_indexArray = _waterbodyLR_indexArray
                    self._waterbodyLR_nIndex = _waterbodyLR_nIndex
                    self._waterbodyLR_Array = _waterbodyLR_Array

        else:
            raise (RuntimeError("No config file provided."))

    def run(self, values: dict):
        """
        Write t-route output values to files. Namely, create restart files (channel and waterbody),
        lastobs files, and flowveldepth files to pass to coastal hydraulics models.
        Parameters
        ----------
        values: dict
            The static and dynamic values for the model.
        Returns
        -------
        """
        output_parameters = self._output_parameters
        da_parameters = self._data_assimilation_parameters
        write_lite_restart = values.get("write_lite_restart")

        if write_lite_restart == 1:
            if output_parameters and output_parameters["lite_restart"] is not None:
                _write_lite_restart(values, self._t0, output_parameters["lite_restart"])
            values["write_lite_restart"] = 0

            lastobs_output_folder = da_parameters.get("streamflow_da", {}).get(
                "lastobs_output_folder", None
            )
            if lastobs_output_folder:
                _write_lastobs(values, self._t0, lastobs_output_folder)

        if output_parameters.get("stream_output", {}):
            t0 = self._t0
            stream_output_directory = output_parameters.get("stream_output").get(
                "stream_output_directory"
            )
            stream_output_type = output_parameters.get("stream_output").get(
                "stream_output_type"
            )
            stream_output_internal_frequency = output_parameters.get(
                "stream_output"
            ).get("stream_output_internal_frequency")
            stream_output_time = output_parameters.get("stream_output").get(
                "stream_output_time"
            )
            write_flowveldepth_netcdf(
                values,
                stream_output_directory,
                t0,
                stream_output_time,
                stream_output_type,
                stream_output_internal_frequency,
            )


# Utility functions -------
def _read_config_file(custom_input_file):
    """
    Read-in data from user-created configuration file.

    Arguments
    ---------
    custom_input_file (str): configuration filepath, .yaml

    Returns
    -------
    bmi_parameters               (dict): Input parameters re bmi configuration
    log_parameters               (dict): Input parameters re logging
    compute_parameters           (dict): Input parameters re computation settings
    data_assimilation_parameters (dict): Input parameters re data assimilation

    """
    with open(custom_input_file) as custom_file:
        data = yaml.load(custom_file, Loader=yaml.SafeLoader)

    troute_configuration = Config(**data)
    config_dict = troute_configuration.dict()

    compute_parameters = config_dict.get("compute_parameters")
    forcing_parameters = compute_parameters.get("forcing_parameters")
    data_assimilation_parameters = compute_parameters.get(
        "data_assimilation_parameters"
    )
    output_parameters = config_dict.get("output_parameters")
    # configure python logger
    log_level_set(config_dict.get("log_parameters"))

    return (
        compute_parameters,
        forcing_parameters,
        data_assimilation_parameters,
        output_parameters,
    )


def _read_timeslice_files(
    filepath,
    dates,
    qc_threshold,
    frequency_secs,
    cpu_pool=1,
    interpolation_limit=59,
):
    # Read files
    observation_df = pd.DataFrame()
    for d in dates:
        f = glob.glob(filepath + "/" + d + "*")

        if f:
            temp_df = xr.open_dataset(f[0])[
                ["stationId", "time", "discharge", "discharge_quality"]
            ].to_dataframe()
            observation_df = pd.concat([observation_df, temp_df])

    if not observation_df.empty:
        observation_df["stationId"] = (
            observation_df["stationId"].str.decode("utf-8").str.strip()
        )
        observation_df["time"] = observation_df["time"].str.decode("utf-8")
        observation_df["discharge_quality"] = observation_df["discharge_quality"] / 100

        # QC/QA and interpolation
        observation_df.loc[observation_df["discharge_quality"] < 0, "discharge"] = (
            np.nan
        )
        observation_df.loc[observation_df["discharge_quality"] > 1, "discharge"] = (
            np.nan
        )
        observation_df.loc[
            observation_df["discharge_quality"] < qc_threshold, "discharge"
        ] = np.nan
        observation_df.loc[observation_df["discharge"] <= 0, "discharge"] = np.nan

        observation_df = (
            observation_df[["stationId", "time", "discharge"]]
            .set_index(["stationId", "time"])
            .unstack(1, fill_value=np.nan)["discharge"]
        )

        # ---- Interpolate USGS observations to the input frequency (frequency_secs)
        observation_df_T = (
            observation_df.transpose()
        )  # transpose, making time the index
        observation_df_T.index = pd.to_datetime(
            observation_df_T.index,
            format="%Y-%m-%d_%H:%M:%S",  # index variable as type datetime
        )

        # specify resampling frequency
        frequency = str(int(frequency_secs / 60)) + "min"

        # interpolate and resample frequency
        buffer_df = observation_df_T.resample(frequency).asfreq()
        with Parallel(n_jobs=cpu_pool) as parallel:
            jobs = []
            interp_chunks = ()
            step = 200
            for a, i in enumerate(range(0, len(observation_df_T.columns), step)):
                start = i
                if (i + step - 1) < buffer_df.shape[1]:
                    stop = i + (step)
                else:
                    stop = buffer_df.shape[1]

                jobs.append(
                    delayed(_interpolate_one)(
                        observation_df_T.iloc[:, start:stop],
                        interpolation_limit,
                        frequency,
                    )
                )

            interp_chunks = parallel(jobs)

        observation_df_T = pd.DataFrame(
            data=np.concatenate(interp_chunks, axis=1),
            columns=buffer_df.columns,
            index=buffer_df.index,
        )

        # re-transpose, making link the index
        observation_df_new = observation_df_T.transpose()

    else:
        observation_df_new = pd.DataFrame()

    return observation_df_new


def _interpolate_one(df, interpolation_limit, frequency):
    interp_out = (
        df.resample("min")
        .interpolate(limit=interpolation_limit, limit_direction="both")
        .resample(frequency)
        .asfreq()
        .to_numpy()
    )
    return interp_out


def _read_timeseries_files(filepath, timeseries_dates, t0, final_persist_datetime):
    # Search for most recent RFC timseries file based on offset hours and lookback window
    # for each location.
    files = glob.glob(filepath + "/*")
    # create temporary dataframe with file names, split up by location and datetime
    df = pd.DataFrame(
        [f.split("/")[-1].split(".") for f in files],
        columns=["Datetime", "dt", "ID", "rfc", "ext"],
    )
    df = df[df["Datetime"].isin(timeseries_dates)][["ID", "Datetime"]]
    # For each location, find the most recent timeseries file (within timeseries window calculated a priori)
    df["Datetime"] = df["Datetime"].apply(lambda _: datetime.strptime(_, "%Y-%m-%d_%H"))
    df = df.groupby("ID").max().reset_index()
    df["Datetime"] = df["Datetime"].dt.strftime("%Y-%m-%d_%H")

    # Loop through list of timeseries files and store relevent information in dataframe.
    file_list = (df["Datetime"] + ".60min." + df["ID"] + ".RFCTimeSeries.ncdf").tolist()
    rfc_df = pd.DataFrame()
    for f in file_list:
        ds = xr.open_dataset(filepath + "/" + f)
        sliceStartTime = datetime.strptime(
            ds.attrs.get("sliceStartTimeUTC"), "%Y-%m-%d_%H:%M:%S"
        )
        sliceTimeResolutionMinutes = ds.attrs.get("sliceTimeResolutionMinutes")
        df = (
            ds.to_dataframe()
            .reset_index()
            .sort_values("forecastInd")[
                [
                    "stationId",
                    "discharges",
                    "synthetic_values",
                    "totalCounts",
                    "timeSteps",
                ]
            ]
        )
        df["Datetime"] = pd.date_range(
            sliceStartTime, periods=df.shape[0], freq=sliceTimeResolutionMinutes + "T"
        )
        # Filter out forecasts that go beyond the rfc_persist_days parameter. This isn't necessary, but removes
        # excess data, keeping the dataframe of observations as small as possible.
        df = df[df["Datetime"] < final_persist_datetime]
        # Locate where t0 is in the timeseries
        df["timeseries_idx"] = df.index[df.Datetime == t0][0]
        df["file"] = f

        # Validate data to determine whether or not it will be used.
        use_rfc = _validate_RFC_data(
            df["stationId"][0],
            df.discharges,
            df.synthetic_values,
            filepath,
            f,
            300,  # NOTE: this is t-route's default timestep. This will need to be verifiied again within t-route...
            False,
        )
        df["use_rfc"] = use_rfc
        df["da_timestep"] = int(sliceTimeResolutionMinutes) * 60

        rfc_df = pd.concat([rfc_df, df])
    rfc_df["stationId"] = rfc_df["stationId"].str.decode("utf-8").str.strip()
    return rfc_df


def _read_lastobs_file(
    lastobsfile,
    station_id="stationId",
    ref_t_attr_id="modelTimeAtOutput",
    time_idx_id="timeInd",
    obs_discharge_id="discharge",
    discharge_nan=-9999.0,
    time_shift=0,
):
    with xr.open_dataset(lastobsfile) as ds:
        gages = ds[station_id].values

        ref_time = datetime.strptime(ds.attrs[ref_t_attr_id], "%Y-%m-%d_%H:%M:%S")

        last_ts = ds[time_idx_id].values[-1]

        df_discharge = (
            ds[obs_discharge_id]
            .to_dataframe()
            .  # discharge to MultiIndex DF
            replace(to_replace=discharge_nan, value=np.nan)
            .  # replace null values with nan
            unstack(level=0)  # unstack to single Index (timeInd)
        )

        last_obs_index = (
            df_discharge.apply(
                pd.Series.last_valid_index
            ).  # index of last non-nan value, each gage
            to_numpy()  # to numpy array
        )
        last_obs_index = np.nan_to_num(last_obs_index, nan=last_ts).astype(int)

        last_observations = []
        lastobs_times = []
        for i, idx in enumerate(last_obs_index):
            last_observations.append(df_discharge.iloc[idx, i])
            lastobs_times.append(ds.time.values[i, idx].decode("utf-8"))

        last_observations = np.array(last_observations)
        lastobs_times = pd.to_datetime(
            np.array(lastobs_times), format="%Y-%m-%d_%H:%M:%S", errors="coerce"
        )

        lastobs_times = (lastobs_times - ref_time).total_seconds()
        lastobs_times = lastobs_times - time_shift

    data_var_dict = {
        "gages": gages,
        "time_since_lastobs": lastobs_times,
        "lastobs_discharge": last_observations,
    }

    lastobs_df = pd.DataFrame(data=data_var_dict)
    lastobs_df["gages"] = lastobs_df["gages"].str.decode("utf-8")

    return lastobs_df


def _read_lite_restart(file):
    """
    Open lite restart pickle files. Can open either waterbody_restart or channel_restart

    Arguments
    -----------
        file (string): File path to lite restart file

    Returns
    ----------
        df (DataFrame): restart states
        t0 (datetime): restart datetime
    """
    # open pickle file to pandas DataFrame
    df = pd.read_pickle(pathlib.Path(file))

    # extract restart time as datetime object
    t0 = df["time"].iloc[0].to_pydatetime()

    return df.drop(columns="time"), t0


def _write_lite_restart(values, t0, lite_restart):
    """
    Save initial conditions dataframes as pickle files

    Arguments
    -----------
        q0 (DataFrame):
        waterbodies_df (DataFrame):
        t0 (datetime.datetime):
        restart_parameters (string):

    Returns
    -----------

    """

    output_directory = lite_restart.get("lite_restart_output_directory", None)
    if output_directory:
        # retrieve/reconstruct t-route produced output dataframes
        q0 = values["q0"]
        q0_ids = values["q0_ids"]
        q0 = pd.DataFrame(
            data=q0.reshape(len(q0_ids), -1), index=q0_ids, columns=["qu0", "qd0", "h0"]
        )

        waterbodies_df = values["waterbody_df"]
        waterbodies_ids = values["waterbody_df_ids"]

        # check for empty waterbody df
        if len(waterbodies_df) == 0:
            waterbodies_df = pd.DataFrame()

        else:
            waterbodies_df = pd.DataFrame(
                data=waterbodies_df.reshape(len(waterbodies_ids), -1),
                index=waterbodies_ids,
                columns=[
                    "ifd",
                    "LkArea",
                    "LkMxE",
                    "OrificeA",
                    "OrificeC",
                    "OrificeE",
                    "WeirC",
                    "WeirE",
                    "WeirL",
                    "lon",
                    "lat",
                    "crs",
                    "qd0",
                    "h0",
                    "index",
                ],
            )

            waterbodies_df.index.name = "lake_id"
            waterbodies_df.drop(columns=["lon", "lat", "crs"], inplace=True)

        timestamp = t0 + timedelta(seconds=values["t-route_model_time"])

        # create restart filenames
        timestamp_str = timestamp.strftime("%Y%m%d%H%M")
        channel_restart_filename = "channel_restart_" + timestamp_str
        waterbody_restart_filename = "waterbody_restart_" + timestamp_str

        q0_out = q0.copy()
        q0_out["time"] = timestamp
        q0_out.to_pickle(
            pathlib.Path.joinpath(output_directory, channel_restart_filename)
        )

        if not waterbodies_df.empty:
            wbody_initial_states = waterbodies_df.loc[:, ["qd0", "h0"]]
            wbody_initial_states["time"] = timestamp
            wbody_initial_states.to_pickle(
                pathlib.Path.joinpath(output_directory, waterbody_restart_filename)
            )


def _write_lastobs(
    values,
    t0,
    lastobs_output_folder=False,
):
    # join gageIDs to lastobs_df
    lastobs_df = values["lastobs_df"]
    lastobs_df_ids = values["lastobs_df_ids"]
    lastobs_df = pd.DataFrame(
        data=lastobs_df.reshape(len(lastobs_df_ids), -1),
        index=lastobs_df_ids,
        columns=["time_since_lastobs", "lastobs_discharge", "gages"],
    )

    # timestamp of last simulation timestep
    modelTimeAtOutput = t0 + timedelta(seconds=values["t-route_model_time"])
    modelTimeAtOutput_str = modelTimeAtOutput.strftime("%Y-%m-%d_%H:%M:%S")

    # timestamp of last observation
    var = [timedelta(seconds=d) for d in lastobs_df.time_since_lastobs.fillna(0)]
    # shorvath (10/18/23): This code was copied from nhd_io.py based on V3 operations.
    # I changed 'modelTimeAtOutput - d' to 'modelTimeAtOutput + d' here because the '-' was
    # creating timestamps that are ahead of the model time which can't be right. I'm not sure
    # if this method was thoroughly checked for V3, but I've only made the update here.
    # TODO: Determine if this should also be changed in nhd_io.py...
    lastobs_timestamp = [modelTimeAtOutput + d for d in var]
    lastobs_timestamp_str = [d.strftime("%Y-%m-%d_%H:%M:%S") for d in lastobs_timestamp]
    lastobs_timestamp_str_array = np.asarray(
        lastobs_timestamp_str, dtype="|S19"
    ).reshape(len(lastobs_timestamp_str), 1)

    # create xarray Dataset similarly structured to WRF-generated lastobs netcdf files
    ds = xr.Dataset(
        {
            "stationId": (["stationIdInd"], lastobs_df["gages"].to_numpy(dtype="|S15")),
            "time": (
                ["stationIdInd", "timeInd"],
                np.asarray(lastobs_timestamp_str_array, dtype="|S19"),
            ),
            "discharge": (
                ["stationIdInd", "timeInd"],
                lastobs_df["lastobs_discharge"]
                .to_numpy()
                .reshape(len(lastobs_df["lastobs_discharge"]), 1),
            ),
        }
    )
    ds.attrs["modelTimeAtOutput"] = modelTimeAtOutput_str

    # write-out LastObs file as netcdf
    if isinstance(lastobs_output_folder, pathlib.Path):
        lastobs_output_folder = str(lastobs_output_folder)
    output_path = pathlib.Path(
        lastobs_output_folder + "/nudgingLastObs." + modelTimeAtOutput_str + ".nc"
    ).resolve()
    ds.to_netcdf(str(output_path))


def write_flowveldepth_netcdf(
    values,
    stream_output_directory,
    t0,
    stream_output_timediff=1,
    stream_output_type=".nc",
    stream_output_internal_frequency=5,
):
    """
    Write the results of flowveldepth and nudge to netcdf- break.
    Arguments
    -------------
    stream_output_directory (Path or string) - directory where file will be created
    flowveldepth (DataFrame) -  including flowrate, velocity, and depth for each time step
    nudge (numpy.ndarray) - nudge data with shape (76, 289)
    usgs_positions_id (array) - Position ids of usgs gages
    """
    ## getting the flatten value of nudge
    flatten_nudge = values.get("nudging")
    nudging_ids = values.get("nudging_ids")
    # reconstruct the df
    num_rows_ndg = len(nudging_ids)
    if num_rows_ndg > 0:
        num_columns_ndg = len(flatten_nudge) // len(nudging_ids)
        nudge = pd.DataFrame(
            flatten_nudge.reshape(num_rows_ndg, num_columns_ndg), index=nudging_ids
        )
    else:
        nudge = pd.DataFrame()

    ## getting the flatten value of flowveldepth
    flatten_fvd = values.get("fvd_results")
    fvd_index = values.get("fvd_index")
    # reconstruct the df
    num_rows_fvd = len(fvd_index)
    num_columns_fvd = len(flatten_fvd) // len(fvd_index)
    flowveldepth = pd.DataFrame(
        flatten_fvd.reshape(num_rows_fvd, num_columns_fvd), index=fvd_index
    )
    # Number of timesteps and features
    nsteps = len(flowveldepth.columns) // 3
    num_features = len(flowveldepth)
    nstep_nc = 12 * stream_output_timediff
    gage, nudge_timesteps = nudge.shape
    column_name = [(i + 1, attr) for i in range(nsteps) for attr in ["q", "v", "d"]]
    flowveldepth.columns = column_name
    # --------- Add 'nudge' column based on usgs_positions_id----------

    # Create a copy of the flowveldepth DataFrame to add 'ndg' columns
    qvd_ndg = flowveldepth.copy()
    # Create a list for names of the columns for nudge values

    ndg_columns = [(j + 1, "ndg") for j in range(nsteps)]
    # renaming nudge columns

    if nudge.empty:
        nudge = pd.DataFrame(columns=ndg_columns)
    else:
        nudge.columns = ndg_columns

    # Left joining qvd and nudge values on index
    qvd_ndg = pd.merge(qvd_ndg, nudge, left_index=True, right_index=True, how="left")

    new_order = [
        (i + 1, attr) for i in range(nsteps) for attr in ["q", "v", "d", "ndg"]
    ]
    # Reorder the columns
    qvd_ndg = qvd_ndg[new_order]

    hr = values.get("t-route_model_time") // 3600 - 1
    # Create time step values based on t0
    time_steps = [t0 + timedelta(hours=hr)]
    time_dim = [
        t * stream_output_internal_frequency * 60
        for t in range(1, int(1 * 60 / stream_output_internal_frequency) + 1)
    ]

    # renaming the columns based on timestamp
    column_name_timeStamp = [
        t0 + timedelta(hours=hr) + timedelta(minutes=(i * 5))
        for i in range(1, nsteps + 1)
    ]
    new_column_name_timeStamp = [
        (attr + "_" + str(cnt + 1) + "_" + times.strftime("%Y%m%d%H%M"))
        for cnt, times in enumerate(column_name_timeStamp)
        for attr in ["q", "v", "d", "ndg"]
    ]
    qvd_ndg.columns = new_column_name_timeStamp

    for counter, i in enumerate(range(0, nsteps, nstep_nc)):
        # Define the range of columns for this file
        start_col = i * 4
        end_col = min((i + nstep_nc) * 4, nsteps * 4)
        selected_col = stream_output_internal_frequency // 5
        # Create a subset DataFrame for the current range of columns
        # subset_df = qvd_ndg.iloc[:, start_col:end_col]
        # Create a list of column names to keep
        columns_to_keep = [
            col
            for col in qvd_ndg.columns[start_col:end_col]
            if int(col.split("_")[1]) % selected_col == 0
        ]
        subset_df = qvd_ndg[columns_to_keep]
        subset_df.columns = [
            "_".join([col.split("_")[0], col.split("_")[2]])
            for col in subset_df.columns
        ]

        # Create the file name based on the current time step
        current_time_step = time_steps[counter].strftime("%Y%m%d%H%M")
        if stream_output_directory:
            if stream_output_type == ".nc":
                file_name = f"{current_time_step}.flowveldepth.nc"

            elif stream_output_type == ".csv":
                file_name = f"{current_time_step}.flowveldepth.csv"
                # Save the data to CSV file
                subset_df.to_csv(f"{stream_output_directory}/{file_name}", index=True)
                LOG.debug(
                    f"Flowveldepth data saved as CSV files in {stream_output_directory}"
                )

            elif stream_output_type == ".pkl":
                file_name = f"{current_time_step}.flowveldepth.pkl"
                # Save the data to Pickle file
                subset_df.to_pickle(f"{stream_output_directory}/{file_name}")
                LOG.debug(
                    f"Flowveldepth data saved as PICKLE files in {stream_output_directory}"
                )

            else:
                print("WRONG FORMAT")

        if stream_output_directory:
            if stream_output_type == ".nc":
                # Open netCDF4 Dataset in write mode
                with netCDF4.Dataset(
                    filename=f"{stream_output_directory}/{file_name}",
                    mode="w",
                    format="NETCDF4",
                ) as ncfile:
                    # ============ DIMENSIONS ===================
                    _ = ncfile.createDimension("feature_id", None)
                    _ = ncfile.createDimension(
                        "time_step (sec)", subset_df.iloc[:, 0::4].shape[1]
                    )
                    _ = ncfile.createDimension("gage", gage)
                    _ = ncfile.createDimension(
                        "nudge_timestep", nudge_timesteps
                    )  # Add dimension for nudge time steps

                    # =========== q,v,d,ndg VARIABLES ===============
                    for counters, var in enumerate(
                        ["flowrate", "velocity", "depth", "nudge"]
                    ):
                        QVD = ncfile.createVariable(
                            varname=var,
                            datatype=np.float32,
                            dimensions=(
                                "feature_id",
                                "time_step (sec)",
                            ),
                        )

                        QVD.units = "m3/s m/s m m3/s"
                        QVD.description = f"Data for {var}"

                        # Prepare data for writing
                        data_array = subset_df.iloc[:, counters::4].to_numpy(
                            dtype=np.float32
                        )

                        # Set data for each feature_id and time_step
                        ncfile.variables[var][:] = data_array
                    feature_id = ncfile.createVariable(
                        varname="feature_id",
                        datatype=np.int32,
                        dimensions=("feature_id",),
                    )
                    feature_id[:] = flowveldepth.index.to_numpy(dtype=np.int32)
                    feature_id.units = "None"
                    feature_id.description = "Feature IDs"
                    ###
                    time_step = ncfile.createVariable(
                        varname="time_step (sec)",
                        datatype=np.int32,
                        dimensions=("time_step (sec)",),
                    )
                    time_step[:] = np.array(
                        time_dim[: subset_df.iloc[:, 0::4].shape[1]], dtype=np.int32
                    )
                    time_step.units = "sec"
                    time_step.description = "time stamp"
                    # =========== GLOBAL ATTRIBUTES ===============
                    ncfile.setncatts(
                        {
                            "TITLE": "OUTPUT FROM T-ROUTE",
                            "Time step (sec)": f"{stream_output_internal_frequency}",
                            "model_initialization_time": t0.strftime(
                                "%Y-%m-%d_%H:%M:%S"
                            ),
                            "model_reference_time": time_steps[counter].strftime(
                                "%Y-%m-%d_%H:%M:%S"
                            ),
                            "comment": f"The file includes {stream_output_timediff} hour data which includes {len(time_dim)} timesteps",
                            "code_version": "",
                        }
                    )
                    LOG.debug(
                        f"Flowveldepth data saved as NetCDF files in {stream_output_directory}"
                    )
